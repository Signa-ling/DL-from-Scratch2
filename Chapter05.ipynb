{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter05 リカレントニューラルネットワーク(RNN)\n",
    "\n",
    "---\n",
    "\n",
    "### 前準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from common.layers import *\n",
    "from common.optimizer import SGD\n",
    "from dataset import ptb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 確率と言語モデル\n",
    "\n",
    "- 言語モデル: 単語の並びに対し, それがどれだけ起こりうるかを確率で評価する\n",
    "  - 音声認識等のアプリケーションで使用可能\n",
    "  - 確率分布に従い新しい文章生成の用途にも利用可能\n",
    "\n",
    "#### 言語モデルを数式で記述\n",
    "- $P(w_1,...,w_m)$: $w_1,...,w_m$までのm個の単語において, $w_1,...,w_m$という順序で単語が出現する確率\n",
    "  - これを事後確率を使用すると以下のように分解できる\n",
    "$$\n",
    "P(w_1,...,w_m) = P(w_m|w_1,...,w_{m-1})P(w_{m-1}|w_1,...w_{m-2})...P(w_3|w_1,w_2)P(w_2|w_1)P(w_1) = \\Pi_{t=1}^mP(w_t|w_1,...w_{t-1})\n",
    "$$\n",
    "\n",
    "- 言語モデルが扱う事後確率: t番目の単語を他ゲットとして, t番目より左側の単語全てをコンテキストとする\n",
    "  - ちなみに$P(w_t|w_1,...,w_{t-1})$を表すモデルを条件付き言語モデルと呼ぶ, またこのモデルを言語モデルと呼ぶ場合もある\n",
    "\n",
    "#### CBOWモデルを言語モデルへ適用 \n",
    "- コンテキストサイズをある値に固定することで近似的に表現するという方法(下の例はコンテキストサイズを2とした場合)\n",
    "  - $P(w_1,...,w_m)=\\Pi_{t=1}^mP(w_t|w_1,...w_{t-1}) \\approx \\Pi_{t=1}^mP(w_t|w_{t-2},w_{t-1})$\n",
    "  - この方法はコンテキストサイズを任意の長さに固定できる\n",
    "  - 一方でそのサイズより左側にある単語情報を無視するため, 指定したサイズよりも前に出現する単語の情報が必要となる文では正答が難しくなる\n",
    "    - CBOWモデルではコンテキスト内の単語の並びが無視される((you, say)と(say, you)は同じコンテキストとして扱われる)為, サイズを大きくしても解決できない\n",
    "    - そこでRNNの出番\n",
    "    \n",
    "---\n",
    "\n",
    "## RNN(Recurent Neural Network)\n",
    "\n",
    "- 再帰NN, 循環NNなどと訳される\n",
    "- Recursive Neural Networkの1種である, これはリカレントニューラルネットワークとは別物\n",
    "- RNNは閉じた経路(ループする経路)を持っており, この経路によって過去の情報を記憶しつつ最新データへと更新される\n",
    "- 各時刻のRNNレイヤは, そのレイヤへの入力と一つ前のRNNレイヤからの出力を受け取り, その2つの情報を基にその時刻の出力が計算される\n",
    "  - これを計算式で表すと以下のようになる\n",
    "\n",
    "$$\n",
    "h_t = \\tanh(h_{t-1}W_h+x_tW_x+b)\n",
    "$$\n",
    "\n",
    "- $x$: 入力($x_t$は行ベクトル)\n",
    "- $h$: 出力($h_{t-1}$は行ベクトル)\n",
    "- $t$: 時刻\n",
    "- $W_x$: 入力$x$を出力$h$に変換するための重み\n",
    "- $W_h$: 一つ前のRNNの出力を次時刻の出力に変換するための重み\n",
    "- $b$: バイアス\n",
    "- $\\tanh$: 双曲線正接関数\n",
    "\n",
    "---\n",
    "\n",
    "### BPTT(Backpropagation Through Time)\n",
    "\n",
    "- RNNは通常の誤差逆伝播法を使用できる(i.e. 順伝播→逆伝播を行うことで目的の勾配を求められる)\n",
    "  - この誤差逆伝播法は,「時間方向に展開したNNの誤差逆伝播法」ということでBPTTと呼ばれる\n",
    "- デメリット\n",
    "  - 時系列データの時間サイズの大きさに比例してBPTTで消費するコンピュータの計算リソースも増加する\n",
    "  - 時間サイズの増加に伴い, 逆伝播時の勾配が不安定になる\n",
    "  \n",
    "- 上記デメリットの対策手法がTruncated BPTT\n",
    "  - 時間軸方向に長いネットワークのつながりを適当な場所で切り取り小さいネットワークを複数つくり, それらに誤差逆伝播法を行う\n",
    "    - 正確に言うと切り取るのはネットワークの逆伝播のつながりのみで, 順伝播のつながりは維持する\n",
    "    - 維持のためにはデータをシーケンシャルに与える, ミニバッチの各バッチでデータを与える開始位置をズラして読み込む必要がある\n",
    "\n",
    "---\n",
    "\n",
    "### RNNレイヤ, Time RNNレイヤの実装\n",
    "\n",
    "- 本書では時系列データをまとめて処理するレイヤにはTimeを頭につけるという命名規則が定義されている\n",
    "  - なのでTステップ分の処理をまとめて行うレイヤをTime RNNレイヤ, Time RNNレイヤ内の1ステップの処理を行うレイヤをRNNレイヤとしている\n",
    "    - RNN $\\in$ Time RNNみたいなイメージ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, x, h_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n",
    "        h_next = np.tanh(t)\n",
    "        \n",
    "        self.cache = (x, h_prev, h_next)\n",
    "        return h_next\n",
    "    \n",
    "    def backward(self, dh_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, h_next = self.cache\n",
    "        \n",
    "        dt = dh_next * (1 - h_next ** 2)\n",
    "        db = np.sum(dt, axis=0)\n",
    "        dWh = np.dot(h_prev.T, dt)\n",
    "        dh_prev = np.dot(dt, Wh.T)\n",
    "        dWx = np.dot(x.T, dt)\n",
    "        dx = np.dot(dt, Wx.T)\n",
    "        \n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "        \n",
    "        return dx, dh_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeRNN:\n",
    "    def __init__(self, Wx, Wh, b, stateful=False):\n",
    "        '''\n",
    "        stateful=Trueであれば状態を持つ事になる → 長い時系列データであっても順伝播を断ち切ること無く伝播させる\n",
    "        stateful=Falseであればforward()が呼ばれる度, 最初のRNNレイヤの隠れ状態をゼロ行列で初期化する\n",
    "        '''\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "        \n",
    "        self.h, self.dh = None, None\n",
    "        self.stateful = stateful\n",
    "    \n",
    "    def set_state(self, h):\n",
    "        self.h = h\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.h = None\n",
    "        \n",
    "    def forward(self, xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        D, H = Wx.shape\n",
    "        \n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "        \n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "            \n",
    "        for t in range(T):\n",
    "            layer = RNN(*self.params)\n",
    "            self.h = layer.forward(xs[:, t, :], self.h)\n",
    "            hs[:, t, :] = self.h\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D, H = Wx.shape\n",
    "        \n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh = 0\n",
    "        grads = [0, 0, 0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh = layer.backward(dhs[:, t, :]+dh)\n",
    "            dxs[:, t, :] = dx\n",
    "            \n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                self.grads[i][...] = grad\n",
    "            self.dh = dh\n",
    "            \n",
    "        return dxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 時系列データを扱うレイヤの実装\n",
    "\n",
    "- この章での目標はRNNを使って言語モデルを実装すること\n",
    "  - 以降RNNによる言語モデルはRNNLM(RNN Language Model)とする\n",
    "\n",
    "- $t$個ある入出力$w, y$の内, $w_i$の出力を$y_i$とするとネットワーク全体では以下の処理を$t$回行っている\n",
    "  - 1.Embedding\n",
    "  - 2.RNN\n",
    "  - 3.Affine\n",
    "  - 4.Softmax(Softmax with Loss)\n",
    "\n",
    "- 各レイヤにおける$t$回の処理をそれぞれのTimeレイヤでまとめる\n",
    "  - 1.Time Embedding\n",
    "  - 2.Time RNN (実装済み)\n",
    "  - 3.Time Affine\n",
    "  - 4.Time Softmax(Softmax with Loss)\n",
    "\n",
    "    - このうち未実装の3つのレイヤを実装する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.layers = None\n",
    "        self.W = W\n",
    "\n",
    "    def forward(self, xs):\n",
    "        N, T = xs.shape\n",
    "        V, D = self.W.shape\n",
    "\n",
    "        out = np.empty((N, T, D), dtype='f')\n",
    "        self.layers = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Embedding(self.W)\n",
    "            out[:, t, :] = layer.forward(xs[:, t])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, D = dout.shape\n",
    "\n",
    "        grad = 0\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            layer.backward(dout[:, t, :])\n",
    "            grad += layer.grads[0]\n",
    "\n",
    "        self.grads[0][...] = grad\n",
    "        return None\n",
    "\n",
    "\n",
    "class TimeAffine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, T, D = x.shape\n",
    "        W, b = self.params\n",
    "\n",
    "        rx = x.reshape(N*T, -1)\n",
    "        out = np.dot(rx, W) + b\n",
    "        self.x = x\n",
    "        return out.reshape(N, T, -1)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        x = self.x\n",
    "        N, T, D = x.shape\n",
    "        W, b = self.params\n",
    "\n",
    "        dout = dout.reshape(N*T, -1)\n",
    "        rx = x.reshape(N*T, -1)\n",
    "\n",
    "        db = np.sum(dout, axis=0)\n",
    "        dW = np.dot(rx.T, dout)\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dx = dx.reshape(*x.shape)\n",
    "\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class TimeSoftmaxWithLoss:\n",
    "    # T個のSoftmax with Lossレイヤで算出されたそれぞれの損失を, 合算し平均を取ったものを最終的損失とする\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "        self.ignore_label = -1\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        N, T, V = xs.shape\n",
    "\n",
    "        if ts.ndim == 3:  # 教師ラベルがone-hotベクトルの場合\n",
    "            ts = ts.argmax(axis=2)\n",
    "\n",
    "        mask = (ts != self.ignore_label)\n",
    "\n",
    "        # バッチ分と時系列分をまとめる（reshape）\n",
    "        xs = xs.reshape(N * T, V)\n",
    "        ts = ts.reshape(N * T)\n",
    "        mask = mask.reshape(N * T)\n",
    "\n",
    "        ys = softmax(xs)\n",
    "        ls = np.log(ys[np.arange(N * T), ts])\n",
    "        ls *= mask  # ignore_labelに該当するデータは損失を0にする\n",
    "        loss = -np.sum(ls)\n",
    "        loss /= mask.sum()\n",
    "\n",
    "        self.cache = (ts, ys, mask, (N, T, V))\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        ts, ys, mask, (N, T, V) = self.cache\n",
    "\n",
    "        dx = ys\n",
    "        dx[np.arange(N * T), ts] -= 1\n",
    "        dx *= dout\n",
    "        dx /= mask.sum()\n",
    "        dx *= mask[:, np.newaxis]  # ignore_labelに該当するデータは勾配を0にする\n",
    "\n",
    "        dx = dx.reshape((N, T, V))\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## RNNLMの学習と評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRnnlm:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        # 重みの初期化\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        rnn_Wx = (rn(D, H) / np.sqrt(D)).astype('f')\n",
    "        rnn_Wh = (rn(H, H) / np.sqrt(H)).astype('f')\n",
    "        rnn_b = np.zeros(H).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "        \n",
    "        # レイヤの生成\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True),\n",
    "            TimeAffine(affine_W, affine_b)\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.rnn_layer = self.layers[1]\n",
    "        \n",
    "        # 全ての重みと勾配をリストにまとめる\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "            \n",
    "    def forward(self, xs, ts):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        loss = self.loss_layer.forward(xs, ts)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.rnn_layer.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 言語モデルの評価\n",
    "\n",
    "- 言語モデルでは過去に与えられた単語から次に出現する単語の確率分布を出力する\n",
    "  - この際の予測性能の良さを評価する指標: パープレキシティ(簡単に言うと確率の逆数)\n",
    "  - パープレキシティは小さいほど良く(最小値は1.0), この値は分岐数を示す\n",
    "  - 入力データが複数の場合のパープレキシティ: $perplexity = e^L$($L$はニューラルネットワークの損失)\n",
    "  \n",
    "#### RNNLMの学習\n",
    "\n",
    "- PTBデータセットを利用して学習を行う\n",
    "  - ただし、本章でのRNNLMは全ての訓練データを対象にすると良い結果を出せない為, PTBデータセットの先頭の1000単語だけを利用する\n",
    "  - 次章で改良 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus size: 1000, vocabulary size: 418\n",
      "| epoch 1 | perp;exity 413.44\n",
      "| epoch 2 | perp;exity 370.55\n",
      "| epoch 3 | perp;exity 270.09\n",
      "| epoch 4 | perp;exity 231.00\n",
      "| epoch 5 | perp;exity 213.25\n",
      "| epoch 6 | perp;exity 207.25\n",
      "| epoch 7 | perp;exity 200.72\n",
      "| epoch 8 | perp;exity 197.06\n",
      "| epoch 9 | perp;exity 191.71\n",
      "| epoch 10 | perp;exity 191.53\n",
      "| epoch 11 | perp;exity 188.42\n",
      "| epoch 12 | perp;exity 190.10\n",
      "| epoch 13 | perp;exity 187.67\n",
      "| epoch 14 | perp;exity 188.12\n",
      "| epoch 15 | perp;exity 187.40\n",
      "| epoch 16 | perp;exity 183.26\n",
      "| epoch 17 | perp;exity 180.95\n",
      "| epoch 18 | perp;exity 177.95\n",
      "| epoch 19 | perp;exity 178.93\n",
      "| epoch 20 | perp;exity 180.68\n",
      "| epoch 21 | perp;exity 178.13\n",
      "| epoch 22 | perp;exity 174.46\n",
      "| epoch 23 | perp;exity 171.31\n",
      "| epoch 24 | perp;exity 173.15\n",
      "| epoch 25 | perp;exity 171.10\n",
      "| epoch 26 | perp;exity 171.62\n",
      "| epoch 27 | perp;exity 166.09\n",
      "| epoch 28 | perp;exity 164.94\n",
      "| epoch 29 | perp;exity 163.73\n",
      "| epoch 30 | perp;exity 158.76\n",
      "| epoch 31 | perp;exity 159.88\n",
      "| epoch 32 | perp;exity 155.40\n",
      "| epoch 33 | perp;exity 157.60\n",
      "| epoch 34 | perp;exity 153.50\n",
      "| epoch 35 | perp;exity 154.05\n",
      "| epoch 36 | perp;exity 148.06\n",
      "| epoch 37 | perp;exity 144.17\n",
      "| epoch 38 | perp;exity 142.83\n",
      "| epoch 39 | perp;exity 138.35\n",
      "| epoch 40 | perp;exity 135.92\n",
      "| epoch 41 | perp;exity 136.56\n",
      "| epoch 42 | perp;exity 130.86\n",
      "| epoch 43 | perp;exity 126.65\n",
      "| epoch 44 | perp;exity 124.92\n",
      "| epoch 45 | perp;exity 121.76\n",
      "| epoch 46 | perp;exity 120.19\n",
      "| epoch 47 | perp;exity 113.83\n",
      "| epoch 48 | perp;exity 109.35\n",
      "| epoch 49 | perp;exity 108.43\n",
      "| epoch 50 | perp;exity 104.80\n",
      "| epoch 51 | perp;exity 101.24\n",
      "| epoch 52 | perp;exity 98.64\n",
      "| epoch 53 | perp;exity 95.51\n",
      "| epoch 54 | perp;exity 94.34\n",
      "| epoch 55 | perp;exity 91.30\n",
      "| epoch 56 | perp;exity 85.44\n",
      "| epoch 57 | perp;exity 83.34\n",
      "| epoch 58 | perp;exity 79.89\n",
      "| epoch 59 | perp;exity 77.64\n",
      "| epoch 60 | perp;exity 74.24\n",
      "| epoch 61 | perp;exity 73.25\n",
      "| epoch 62 | perp;exity 70.07\n",
      "| epoch 63 | perp;exity 65.93\n",
      "| epoch 64 | perp;exity 63.65\n",
      "| epoch 65 | perp;exity 62.64\n",
      "| epoch 66 | perp;exity 58.95\n",
      "| epoch 67 | perp;exity 57.35\n",
      "| epoch 68 | perp;exity 53.12\n",
      "| epoch 69 | perp;exity 51.49\n",
      "| epoch 70 | perp;exity 50.26\n",
      "| epoch 71 | perp;exity 47.17\n",
      "| epoch 72 | perp;exity 45.46\n",
      "| epoch 73 | perp;exity 43.60\n",
      "| epoch 74 | perp;exity 42.12\n",
      "| epoch 75 | perp;exity 41.18\n",
      "| epoch 76 | perp;exity 37.54\n",
      "| epoch 77 | perp;exity 36.68\n",
      "| epoch 78 | perp;exity 34.80\n",
      "| epoch 79 | perp;exity 33.08\n",
      "| epoch 80 | perp;exity 31.72\n",
      "| epoch 81 | perp;exity 30.80\n",
      "| epoch 82 | perp;exity 29.72\n",
      "| epoch 83 | perp;exity 27.59\n",
      "| epoch 84 | perp;exity 26.40\n",
      "| epoch 85 | perp;exity 25.27\n",
      "| epoch 86 | perp;exity 24.01\n",
      "| epoch 87 | perp;exity 23.36\n",
      "| epoch 88 | perp;exity 21.82\n",
      "| epoch 89 | perp;exity 20.69\n",
      "| epoch 90 | perp;exity 20.04\n",
      "| epoch 91 | perp;exity 19.28\n",
      "| epoch 92 | perp;exity 18.68\n",
      "| epoch 93 | perp;exity 17.70\n",
      "| epoch 94 | perp;exity 16.99\n",
      "| epoch 95 | perp;exity 16.21\n",
      "| epoch 96 | perp;exity 15.58\n",
      "| epoch 97 | perp;exity 14.93\n",
      "| epoch 98 | perp;exity 13.92\n",
      "| epoch 99 | perp;exity 13.53\n",
      "| epoch 100 | perp;exity 12.89\n"
     ]
    }
   ],
   "source": [
    "# ハイパーパラメータの設定\n",
    "batch_size = 10\n",
    "wordvec_size = 100\n",
    "hidden_size = 100 # RNNの隠れ状態ベクトルの要素数\n",
    "time_size = 5 # Truncated BPTTの展開する時間サイズ\n",
    "lr = 0.1\n",
    "max_epoch = 100\n",
    "\n",
    "# 学習データの読み込み(データセットを小さくする)\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "corpus_size= 1000\n",
    "corpus = corpus[:corpus_size]\n",
    "vocab_size = int(max(corpus) + 1)\n",
    "\n",
    "xs = corpus[:-1] # 入力\n",
    "ts = corpus[1:] # 出力(今日調べる)\n",
    "data_size = len(xs)\n",
    "print('corpus size: %d, vocabulary size: %d' % (corpus_size, vocab_size))\n",
    "\n",
    "# 学習時に使用する変数\n",
    "max_iters = data_size // (batch_size * time_size)\n",
    "time_idx = 0\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "ppl_list = []\n",
    "\n",
    "# モデルの生成\n",
    "model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = SGD(lr)\n",
    "\n",
    "# ミニバッチの各サンプルの読み込み始点を計算\n",
    "# 各バッチでデータを与える開始位置をズラして読み込む必要があるため\n",
    "jump = (corpus_size - 1) // batch_size\n",
    "offsets = [i * jump for i in range(batch_size)]\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    for iter in range(max_iters):\n",
    "        # ミニバッチ取得\n",
    "        # データをシーケンシャルに与える\n",
    "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
    "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
    "        \n",
    "        for t in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                batch_x[i, t] = xs[(offset + time_idx) % data_size]\n",
    "                batch_t[i, t] = ts[(offset + time_idx) % data_size]\n",
    "            time_idx += 1\n",
    "        \n",
    "        # 勾配を求め, パラメータ更新\n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        optimizer.update(model.params, model.grads)\n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "    \n",
    "    # epoch毎にパープレキシティの取得\n",
    "    ppl = np.exp(total_loss / loss_count)\n",
    "    print('| epoch %d | perp;exity %.2f' %(epoch+1, ppl))\n",
    "    ppl_list.append(float(ppl))\n",
    "    total_loss, loss_count = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# グラフの描画\n",
    "x = np.arange(len(ppl_list))\n",
    "plt.plot(x, ppl_list, label='train')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('perplexity')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('DL-from-Scratch2': venv)",
   "language": "python",
   "name": "python37364bitdlfromscratch2venvf2e9687bc0124088b33a9bcbeb1d9a2b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter08\n",
    "\n",
    "---\n",
    "\n",
    "### 前準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from common.np import *\n",
    "from common.layers import Softmax\n",
    "from common.optimizer import Adam\n",
    "from common.time_layers import *\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "from dataset import sequence\n",
    "from etc.seq2seq import Encoder, Seq2seq\n",
    "from etc.peeky_seq2seq import PeekySeq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "- 注意機構(Attention mechanism)\n",
    "- seq2seqの根本的問題を解決する改良案\n",
    "- seq2seqの問題点\n",
    "  - Encoderの出力が固定長のベクトルであること\n",
    "    - 情報が大きくても無理やり圧縮するため必要な情報が溢れる\n",
    "\n",
    "---\n",
    "\n",
    "### Encoderの改良案\n",
    "\n",
    "- 今までは最後のLSTMの隠れ状態ベクトルを使用していた\n",
    "  - 今回は各時刻のLSTMの隠れ状態ベクトルを使用する\n",
    "    - これによって入力された単語列と同じ長さのベクトルを得れる\n",
    "    - 各時刻の隠れ状態には直前に入力された単語の情報が多く含まれている\n",
    "      - Encorderの出力$hs$は各単語に対応したベクトルの集合体となる\n",
    "\n",
    "---\n",
    "\n",
    "### Decoderの改良案\n",
    "\n",
    "- Encoderの出力を改良したためこちらも改良する必要がある\n",
    "- 翻訳先の単語と対応関係にある翻訳元の単語の情報(アライメント)を抜き出す\n",
    "  - 即ち, 必要情報だけに注意を向け, その情報から時系列変換を行う → Attention  \n",
    "- 選ぶために各単語の重要度を表す重み$a$とベクトル$hs$による重み付き和を求める\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, a):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ar = a.reshape(N, T, 1)#.repeat(T, axis=1)\n",
    "        t = hs * ar\n",
    "        c = np.sum(t, axis=1)\n",
    "\n",
    "        self.cache = (hs, ar)\n",
    "        return c\n",
    "\n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        dar = dt * hs\n",
    "        dhs = dt * ar\n",
    "        da = np.sum(dar, axis=2)\n",
    "\n",
    "        return dhs, da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "- 各単語の重要度を表す重み$a$があれば上記によってコンテキストベクトルを得れる\n",
    "  - $a$はどう求めるか？\n",
    "- DecoderのLSTMレイヤの隠れ状態ベクトル$h$と$hs$の各単語ベクトルの類似度を数値で求める → ベクトルの内積を用いる\n",
    "  - 求めたスコア$s$を正規化するためにSoftmax関数を適用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        hr = h.reshape(N, 1, H)#.repeat(T, axis=1)\n",
    "        t = hs * hr\n",
    "        s = np.sum(t, axis=2)\n",
    "        a = self.softmax.forward(s)\n",
    "\n",
    "        self.cache = (hs, hr)\n",
    "        return a\n",
    "\n",
    "    def backward(self, da):\n",
    "        hs, hr = self.cache\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ds = self.softmax.backward(da)\n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        dhs = dt * hr\n",
    "        dhr = dt * hs\n",
    "        dh = np.sum(dhr, axis=1)\n",
    "\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "- 上の2つのレイヤをAttentionレイヤとして1つにまとめる\n",
    "- ここまでの流れ\n",
    "  - Decoderのその時間のLSTMレイヤの出力$h$と$hs$を用いてAttention Weightレイヤで重み$a$を得る\n",
    "  - $a$と$hs$によってWeight Sumレイヤで重み付き和を計算しAffineレイヤに向けて出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h)\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        self.attention_weight = a\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        dhs = dhs0 + dhs1\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "- 時系列方向に広がる複数のAttentionレイヤをTime Attentionレイヤとしてまとめる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "        \n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        N, T, H = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec)\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = Attention()\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:, t, :])\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        N, T, H = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout[:, t, :])\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:, t, :] = dh\n",
    "        \n",
    "        return dhs_enc, dhs_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Attention付きseq2seqの実装\n",
    "\n",
    "※前章のseq2seqを使用するが再実装が手間なのでフォルダから同クラスを呼び出すことにした"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionEncoder(Encoder):\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        return hs\n",
    "    \n",
    "    def backward(self, dhs):\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.attention = TimeAttention()\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, enc_hs):\n",
    "        h = enc_hs[:,-1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        dec_hs = self.lstm.forward(out)\n",
    "        c = self.attention.forward(enc_hs, dec_hs)\n",
    "        out = np.concatenate((c, dec_hs), axis=2)\n",
    "        score = self.affine.forward(out)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        N, T, H2 = dout.shape\n",
    "        H = H2 // 2\n",
    "\n",
    "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]\n",
    "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
    "        ddec_hs = ddec_hs0 + ddec_hs1\n",
    "        dout = self.lstm.backward(ddec_hs)\n",
    "        dh = self.lstm.dh\n",
    "        denc_hs[:, -1] += dh\n",
    "        self.embed.backward(dout)\n",
    "\n",
    "        return denc_hs\n",
    "\n",
    "    def generate(self, enc_hs, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        h = enc_hs[:, -1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array([sample_id]).reshape((1, 1))\n",
    "\n",
    "            out = self.embed.forward(x)\n",
    "            dec_hs = self.lstm.forward(out)\n",
    "            c = self.attention.forward(enc_hs, dec_hs)\n",
    "            out = np.concatenate((c, dec_hs), axis=2)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(sample_id)\n",
    "\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        args = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = AttentionEncoder(*args)\n",
    "        self.decoder = AttentionDecoder(*args)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "        \n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Attentionの評価\n",
    "\n",
    "- 翻訳データセットの有名なもの: WMT\n",
    "  - 英語とフランス語, または英語とドイツ語が対となったデータセット\n",
    "  - 多くの研究でベンチマークとして利用されているが容量が20GB以上と大きく気軽に利用できない\n",
    "\n",
    "- 今回は日付フォーマットを変更する問題に取り組む\n",
    "  - 様々な形状の日付フォーマットを\"YYYY-MM-DD\"の形状にする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 1 / 351 | time 0[s] | loss 4.08\n",
      "| epoch 1 |  iter 21 / 351 | time 13[s] | loss 3.09\n",
      "| epoch 1 |  iter 41 / 351 | time 26[s] | loss 1.90\n",
      "| epoch 1 |  iter 61 / 351 | time 39[s] | loss 1.72\n",
      "| epoch 1 |  iter 81 / 351 | time 51[s] | loss 1.46\n",
      "| epoch 1 |  iter 101 / 351 | time 64[s] | loss 1.19\n",
      "| epoch 1 |  iter 121 / 351 | time 77[s] | loss 1.14\n",
      "| epoch 1 |  iter 141 / 351 | time 90[s] | loss 1.09\n",
      "| epoch 1 |  iter 161 / 351 | time 103[s] | loss 1.06\n",
      "| epoch 1 |  iter 181 / 351 | time 115[s] | loss 1.04\n",
      "| epoch 1 |  iter 201 / 351 | time 128[s] | loss 1.03\n",
      "| epoch 1 |  iter 221 / 351 | time 141[s] | loss 1.02\n",
      "| epoch 1 |  iter 241 / 351 | time 153[s] | loss 1.02\n",
      "| epoch 1 |  iter 261 / 351 | time 166[s] | loss 1.01\n",
      "| epoch 1 |  iter 281 / 351 | time 178[s] | loss 1.00\n",
      "| epoch 1 |  iter 301 / 351 | time 191[s] | loss 1.00\n",
      "| epoch 1 |  iter 321 / 351 | time 204[s] | loss 1.00\n",
      "| epoch 1 |  iter 341 / 351 | time 216[s] | loss 1.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1978-08-11\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1978-08-11\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1978-08-11\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1978-08-11\n",
      "---\n",
      "val acc 0.000%\n",
      "| epoch 2 |  iter 1 / 351 | time 0[s] | loss 1.00\n",
      "| epoch 2 |  iter 21 / 351 | time 12[s] | loss 1.00\n",
      "| epoch 2 |  iter 41 / 351 | time 25[s] | loss 0.99\n",
      "| epoch 2 |  iter 61 / 351 | time 37[s] | loss 0.99\n",
      "| epoch 2 |  iter 81 / 351 | time 49[s] | loss 0.99\n",
      "| epoch 2 |  iter 101 / 351 | time 62[s] | loss 0.99\n",
      "| epoch 2 |  iter 121 / 351 | time 74[s] | loss 0.99\n",
      "| epoch 2 |  iter 141 / 351 | time 87[s] | loss 0.98\n",
      "| epoch 2 |  iter 161 / 351 | time 99[s] | loss 0.98\n",
      "| epoch 2 |  iter 181 / 351 | time 112[s] | loss 0.97\n",
      "| epoch 2 |  iter 201 / 351 | time 124[s] | loss 0.95\n",
      "| epoch 2 |  iter 221 / 351 | time 137[s] | loss 0.94\n",
      "| epoch 2 |  iter 241 / 351 | time 149[s] | loss 0.90\n",
      "| epoch 2 |  iter 261 / 351 | time 161[s] | loss 0.83\n",
      "| epoch 2 |  iter 281 / 351 | time 173[s] | loss 0.74\n",
      "| epoch 2 |  iter 301 / 351 | time 186[s] | loss 0.66\n",
      "| epoch 2 |  iter 321 / 351 | time 198[s] | loss 0.58\n",
      "| epoch 2 |  iter 341 / 351 | time 210[s] | loss 0.46\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 2006-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 2007-08-09\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1983-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 2016-11-08\n",
      "---\n",
      "val acc 51.640%\n",
      "| epoch 3 |  iter 1 / 351 | time 0[s] | loss 0.35\n",
      "| epoch 3 |  iter 21 / 351 | time 13[s] | loss 0.30\n",
      "| epoch 3 |  iter 41 / 351 | time 25[s] | loss 0.21\n",
      "| epoch 3 |  iter 61 / 351 | time 38[s] | loss 0.14\n",
      "| epoch 3 |  iter 81 / 351 | time 50[s] | loss 0.09\n",
      "| epoch 3 |  iter 101 / 351 | time 63[s] | loss 0.07\n",
      "| epoch 3 |  iter 121 / 351 | time 75[s] | loss 0.05\n",
      "| epoch 3 |  iter 141 / 351 | time 88[s] | loss 0.04\n",
      "| epoch 3 |  iter 161 / 351 | time 100[s] | loss 0.03\n",
      "| epoch 3 |  iter 181 / 351 | time 113[s] | loss 0.03\n",
      "| epoch 3 |  iter 201 / 351 | time 125[s] | loss 0.02\n",
      "| epoch 3 |  iter 221 / 351 | time 138[s] | loss 0.02\n",
      "| epoch 3 |  iter 241 / 351 | time 150[s] | loss 0.02\n",
      "| epoch 3 |  iter 261 / 351 | time 163[s] | loss 0.01\n",
      "| epoch 3 |  iter 281 / 351 | time 175[s] | loss 0.01\n",
      "| epoch 3 |  iter 301 / 351 | time 188[s] | loss 0.01\n",
      "| epoch 3 |  iter 321 / 351 | time 200[s] | loss 0.01\n",
      "| epoch 3 |  iter 341 / 351 | time 213[s] | loss 0.01\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 99.900%\n",
      "| epoch 4 |  iter 1 / 351 | time 0[s] | loss 0.01\n",
      "| epoch 4 |  iter 21 / 351 | time 13[s] | loss 0.01\n",
      "| epoch 4 |  iter 41 / 351 | time 26[s] | loss 0.01\n",
      "| epoch 4 |  iter 61 / 351 | time 39[s] | loss 0.01\n",
      "| epoch 4 |  iter 81 / 351 | time 53[s] | loss 0.01\n",
      "| epoch 4 |  iter 101 / 351 | time 66[s] | loss 0.01\n",
      "| epoch 4 |  iter 121 / 351 | time 79[s] | loss 0.00\n",
      "| epoch 4 |  iter 141 / 351 | time 91[s] | loss 0.01\n",
      "| epoch 4 |  iter 161 / 351 | time 105[s] | loss 0.00\n",
      "| epoch 4 |  iter 181 / 351 | time 118[s] | loss 0.00\n",
      "| epoch 4 |  iter 201 / 351 | time 131[s] | loss 0.00\n",
      "| epoch 4 |  iter 221 / 351 | time 143[s] | loss 0.00\n",
      "| epoch 4 |  iter 241 / 351 | time 155[s] | loss 0.00\n",
      "| epoch 4 |  iter 261 / 351 | time 168[s] | loss 0.00\n",
      "| epoch 4 |  iter 281 / 351 | time 182[s] | loss 0.00\n",
      "| epoch 4 |  iter 301 / 351 | time 195[s] | loss 0.00\n",
      "| epoch 4 |  iter 321 / 351 | time 208[s] | loss 0.00\n",
      "| epoch 4 |  iter 341 / 351 | time 221[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 99.900%\n",
      "| epoch 5 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 5 |  iter 21 / 351 | time 12[s] | loss 0.00\n",
      "| epoch 5 |  iter 41 / 351 | time 25[s] | loss 0.00\n",
      "| epoch 5 |  iter 61 / 351 | time 37[s] | loss 0.00\n",
      "| epoch 5 |  iter 81 / 351 | time 49[s] | loss 0.00\n",
      "| epoch 5 |  iter 101 / 351 | time 62[s] | loss 0.00\n",
      "| epoch 5 |  iter 121 / 351 | time 74[s] | loss 0.00\n",
      "| epoch 5 |  iter 141 / 351 | time 86[s] | loss 0.00\n",
      "| epoch 5 |  iter 161 / 351 | time 99[s] | loss 0.00\n",
      "| epoch 5 |  iter 181 / 351 | time 111[s] | loss 0.00\n",
      "| epoch 5 |  iter 201 / 351 | time 123[s] | loss 0.00\n",
      "| epoch 5 |  iter 221 / 351 | time 135[s] | loss 0.00\n",
      "| epoch 5 |  iter 241 / 351 | time 148[s] | loss 0.00\n",
      "| epoch 5 |  iter 261 / 351 | time 160[s] | loss 0.00\n",
      "| epoch 5 |  iter 281 / 351 | time 173[s] | loss 0.00\n",
      "| epoch 5 |  iter 301 / 351 | time 185[s] | loss 0.00\n",
      "| epoch 5 |  iter 321 / 351 | time 198[s] | loss 0.00\n",
      "| epoch 5 |  iter 341 / 351 | time 210[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 99.920%\n",
      "| epoch 6 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 6 |  iter 21 / 351 | time 13[s] | loss 0.00\n",
      "| epoch 6 |  iter 41 / 351 | time 25[s] | loss 0.00\n",
      "| epoch 6 |  iter 61 / 351 | time 38[s] | loss 0.00\n",
      "| epoch 6 |  iter 81 / 351 | time 50[s] | loss 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 6 |  iter 101 / 351 | time 62[s] | loss 0.00\n",
      "| epoch 6 |  iter 121 / 351 | time 75[s] | loss 0.00\n",
      "| epoch 6 |  iter 141 / 351 | time 87[s] | loss 0.00\n",
      "| epoch 6 |  iter 161 / 351 | time 100[s] | loss 0.00\n",
      "| epoch 6 |  iter 181 / 351 | time 112[s] | loss 0.00\n",
      "| epoch 6 |  iter 201 / 351 | time 125[s] | loss 0.00\n",
      "| epoch 6 |  iter 221 / 351 | time 137[s] | loss 0.00\n",
      "| epoch 6 |  iter 241 / 351 | time 150[s] | loss 0.00\n",
      "| epoch 6 |  iter 261 / 351 | time 162[s] | loss 0.00\n",
      "| epoch 6 |  iter 281 / 351 | time 174[s] | loss 0.00\n",
      "| epoch 6 |  iter 301 / 351 | time 187[s] | loss 0.00\n",
      "| epoch 6 |  iter 321 / 351 | time 199[s] | loss 0.00\n",
      "| epoch 6 |  iter 341 / 351 | time 212[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 99.920%\n",
      "| epoch 7 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 7 |  iter 21 / 351 | time 13[s] | loss 0.00\n",
      "| epoch 7 |  iter 41 / 351 | time 25[s] | loss 0.00\n",
      "| epoch 7 |  iter 61 / 351 | time 38[s] | loss 0.00\n",
      "| epoch 7 |  iter 81 / 351 | time 50[s] | loss 0.00\n",
      "| epoch 7 |  iter 101 / 351 | time 63[s] | loss 0.00\n",
      "| epoch 7 |  iter 121 / 351 | time 75[s] | loss 0.00\n",
      "| epoch 7 |  iter 141 / 351 | time 88[s] | loss 0.00\n",
      "| epoch 7 |  iter 161 / 351 | time 100[s] | loss 0.00\n",
      "| epoch 7 |  iter 181 / 351 | time 113[s] | loss 0.00\n",
      "| epoch 7 |  iter 201 / 351 | time 125[s] | loss 0.00\n",
      "| epoch 7 |  iter 221 / 351 | time 138[s] | loss 0.00\n",
      "| epoch 7 |  iter 241 / 351 | time 151[s] | loss 0.00\n",
      "| epoch 7 |  iter 261 / 351 | time 163[s] | loss 0.00\n",
      "| epoch 7 |  iter 281 / 351 | time 176[s] | loss 0.00\n",
      "| epoch 7 |  iter 301 / 351 | time 188[s] | loss 0.00\n",
      "| epoch 7 |  iter 321 / 351 | time 201[s] | loss 0.00\n",
      "| epoch 7 |  iter 341 / 351 | time 214[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 100.000%\n",
      "| epoch 8 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 8 |  iter 21 / 351 | time 13[s] | loss 0.00\n",
      "| epoch 8 |  iter 41 / 351 | time 25[s] | loss 0.00\n",
      "| epoch 8 |  iter 61 / 351 | time 38[s] | loss 0.00\n",
      "| epoch 8 |  iter 81 / 351 | time 50[s] | loss 0.00\n",
      "| epoch 8 |  iter 101 / 351 | time 62[s] | loss 0.00\n",
      "| epoch 8 |  iter 121 / 351 | time 75[s] | loss 0.00\n",
      "| epoch 8 |  iter 141 / 351 | time 87[s] | loss 0.00\n",
      "| epoch 8 |  iter 161 / 351 | time 99[s] | loss 0.00\n",
      "| epoch 8 |  iter 181 / 351 | time 111[s] | loss 0.00\n",
      "| epoch 8 |  iter 201 / 351 | time 124[s] | loss 0.00\n",
      "| epoch 8 |  iter 221 / 351 | time 136[s] | loss 0.00\n",
      "| epoch 8 |  iter 241 / 351 | time 149[s] | loss 0.00\n",
      "| epoch 8 |  iter 261 / 351 | time 161[s] | loss 0.00\n",
      "| epoch 8 |  iter 281 / 351 | time 174[s] | loss 0.00\n",
      "| epoch 8 |  iter 301 / 351 | time 186[s] | loss 0.00\n",
      "| epoch 8 |  iter 321 / 351 | time 198[s] | loss 0.00\n",
      "| epoch 8 |  iter 341 / 351 | time 211[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 100.000%\n",
      "| epoch 9 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 9 |  iter 21 / 351 | time 13[s] | loss 0.00\n",
      "| epoch 9 |  iter 41 / 351 | time 25[s] | loss 0.00\n",
      "| epoch 9 |  iter 61 / 351 | time 37[s] | loss 0.00\n",
      "| epoch 9 |  iter 81 / 351 | time 50[s] | loss 0.00\n",
      "| epoch 9 |  iter 101 / 351 | time 62[s] | loss 0.00\n",
      "| epoch 9 |  iter 121 / 351 | time 74[s] | loss 0.00\n",
      "| epoch 9 |  iter 141 / 351 | time 87[s] | loss 0.00\n",
      "| epoch 9 |  iter 161 / 351 | time 99[s] | loss 0.00\n",
      "| epoch 9 |  iter 181 / 351 | time 111[s] | loss 0.00\n",
      "| epoch 9 |  iter 201 / 351 | time 124[s] | loss 0.00\n",
      "| epoch 9 |  iter 221 / 351 | time 136[s] | loss 0.00\n",
      "| epoch 9 |  iter 241 / 351 | time 148[s] | loss 0.00\n",
      "| epoch 9 |  iter 261 / 351 | time 161[s] | loss 0.00\n",
      "| epoch 9 |  iter 281 / 351 | time 173[s] | loss 0.00\n",
      "| epoch 9 |  iter 301 / 351 | time 186[s] | loss 0.00\n",
      "| epoch 9 |  iter 321 / 351 | time 198[s] | loss 0.00\n",
      "| epoch 9 |  iter 341 / 351 | time 210[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 100.000%\n",
      "| epoch 10 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 10 |  iter 21 / 351 | time 13[s] | loss 0.00\n",
      "| epoch 10 |  iter 41 / 351 | time 25[s] | loss 0.00\n",
      "| epoch 10 |  iter 61 / 351 | time 37[s] | loss 0.00\n",
      "| epoch 10 |  iter 81 / 351 | time 50[s] | loss 0.00\n",
      "| epoch 10 |  iter 101 / 351 | time 62[s] | loss 0.00\n",
      "| epoch 10 |  iter 121 / 351 | time 75[s] | loss 0.00\n",
      "| epoch 10 |  iter 141 / 351 | time 87[s] | loss 0.00\n",
      "| epoch 10 |  iter 161 / 351 | time 99[s] | loss 0.00\n",
      "| epoch 10 |  iter 181 / 351 | time 112[s] | loss 0.00\n",
      "| epoch 10 |  iter 201 / 351 | time 124[s] | loss 0.00\n",
      "| epoch 10 |  iter 221 / 351 | time 137[s] | loss 0.00\n",
      "| epoch 10 |  iter 241 / 351 | time 149[s] | loss 0.00\n",
      "| epoch 10 |  iter 261 / 351 | time 162[s] | loss 0.00\n",
      "| epoch 10 |  iter 281 / 351 | time 175[s] | loss 0.00\n",
      "| epoch 10 |  iter 301 / 351 | time 187[s] | loss 0.00\n",
      "| epoch 10 |  iter 321 / 351 | time 200[s] | loss 0.00\n",
      "| epoch 10 |  iter 341 / 351 | time 212[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 100.000%\n"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('/date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 10\n",
    "max_grad = 5.0\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1, batch_size=batch_size, max_grad=max_grad)\n",
    "    \n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct, id_to_char, verbose, is_reverse=True)\n",
    "    \n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('val acc %.3f%%' % (acc * 100))\n",
    "\n",
    "model.save_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(len(acc_list))\n",
    "plt.plot(x, acc_list, marker='o')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('DL-from-Scratch2': venv)",
   "language": "python",
   "name": "python37364bitdlfromscratch2venvf2e9687bc0124088b33a9bcbeb1d9a2b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
